<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
  <title>LOCKSS: Using the Plugin Generation Tool</title>
  <meta name="GENERATOR" content="Quanta Plus">
</head>
<body>
<h2 style="text-align: center;">Using The LOCKSS Plugin Generation Tool</h2>
<h3 style="text-align: center;">The LOCKSS Team</h3>
<h3 style="text-align: center;">6/7/04</h3>
<h4>Introduction</h4>
Every journal that LOCKSS might preserve has some peculiarities; some
are more peculiar than others. The LOCKSS daemon gets its knowledge of
these peculiarities, be they large or small, from a <span
 style="font-style: italic;">plugin</span>. The plugin for a complex
journal is written mostly in Java, but for most simple journals the
plugin is an XML file which is interpreted by a generic, or <span
 style="font-style: italic;">definable</span>, plugin written by the
LOCKSS team. Here we introduce the LOCKSS plugin generation tool, which
provides a user interface that allows
you to create and test a plugin for most simple journals with no
programming. The user provides input describing the chosen journal, the
tool outputs a definition of the plugin for the chosen journal in XML.
In cases where a single publishing platform, for example HighWire
Press, publishes a large number of journals a single (complex) plugin
will support all of the journals. This document focuses on the simpler
journals, whose plugin is within the capabilities of the plugin
generation tool, so we assume below that there is a single plugin for
each journal.<br>
<br>
Eventually, the LOCKSS daemon will obtain this XML file via a <span
 style="font-style: italic;">plugin registry</span>, a search facility
linking bibliographic information about journals to the appropriate
plugin. Until the plugin registry is working, the XML files defining
plugins must be e-mailed to lockssbeta "at" lockss.org. After testing,
the LOCKSS team will include them in the daemon distribution.<br>
<br>
One major function of the plugin for a journal is to divide the
journal's content into manageable chunks called <span
 style="font-style: italic;">archival units</span> (AUs). Typically, an
AU will consist of a year's run of a journal, or a volume. Among the
information the plugin needs about an AU is its crawl rules. These tell
the LOCKSS web crawler where to stop when it is trying to find newly
published content to collect.&nbsp; Other information includes the
publisher manifest page, which tells the crawler where to start,&nbsp;
and the crawl interval, which tells it how often to start.&nbsp; We use
examples of some real journals to show how you find this information
from t he journal's web site, how you feed it into the tool to generate
a suitable plugin, and how you test the plugin to be sure you have the
information correct.<br>
<h4>Obtaining and Running The Tool</h4>
The plugin tool is available in two versions: <a href="plugintool.tgz">plugintool.tgz</a>
for Unix and <a href="plugintool.zip">plugintool.zip</a> for Windows.
Download and unpack the appropriate one into a directory (folder) and
invoke the script in that directory that runs the tool, runtool for
Unix and runtool.bat for Windows. You may have to set the JAVA_HOME
environment variable to point to an installed version of Java 1.4.<br>
<ul>
</ul>
<h4>Before You Start Clicking<br>
</h4>
<img src="initial-window.gif" title="" alt="Initial window"
 style="width: 370px; height: 293px;" align="right">When the generator
starts you will see a window which contains the
fields needed to define the
plug-in, like this: <br clear="all">
The
pull-down menus for this window are:<br>
<ul>
  <li>File -&gt; New, Open, Save, Save As, Exit - these allow you to
load and save the XML files defining the plugin.<br>
  </li>
  <li>Edit -&gt; Cut, Copy, Paste, Delete - these operate on editable
text fields.<br>
  </li>
  <li>Plugin -&gt; Expert Mode, Test Crawl Rules ..., Test Filters -
these are described below.</li>
  <li>Help -&gt; About - Brings up version and copyright info.<br>
  </li>
</ul>
Expert Mode allows more control over the plugin, the details are <a
 href="#ExpertMode">here</a>. In most cases you will not need it.
Instead you
will define your plugin by filling in the fields in the window in turn.
They are:<br>
<ul>
  <li>Plugin Name: The human-readable name that the plugin will appear
under in
the
configuration menu of the LOCKSS administrative user interface. Spaces
are OK here.</li>
  <li>Plugin ID:&nbsp; the Java class name for the plugin.<br>
  </li>
  <li>Plugin Version: A version number that differentiates different
versions of the same plugin.&nbsp; It should start at 1.<br>
  </li>
  <li>Configuration Parameters:&nbsp; This defines the set of
parameters that the plugin will use to identify and differentiate
between each AU that uses the plugin.<br>
  </li>
  <li>AU Name Template: This template tells the plugin how to use the
Configuration Parameters to create the name of
each AU using the plugin used by the LOCKSS administrative UI.</li>
  <li>Start URL Template: This template tells the plugin where on the
publisher's web site to find the <span style="font-style: italic;">publisher
manifest page</span> for each AU of the journal.</li>
  <li>Crawl Rules: These rules define the boundaries of an <span
 style="font-style: italic;">archival unit</span> (AU) in the journal's
web site. An AU is normally a year's run or a volume of the journal.<br>
  </li>
  <li>Pause Time Between Fetches: The time for which the LOCKSS daemon
waits after fetching each page from the publisher's web site.<br>
  </li>
  <li>New Content Crawl Interval: The time between attempts by the
LOCKSS daemon to find new content on the publisher's web site.</li>
</ul>
<h4>Example Journal</h4>
We will use the real journal <a href="http://disputatio.com">Disputatio</a>
as an example. Its "home page" is at <a href="http://disputatio.com">http://disputatio.com</a>.
Its publisher manifest page for 2004 is at <a
 href="http://disputatio.com/lockss2004.html">http://disputatio.com/lockss2004.html</a>.
A typical article from 2004 is at <a
 href="http://disputatio.com/articles/016-3.pdf">http://disputatio.com/articles/016-3.pdf</a>
- it is the third article in issue number 16 of the journal. That's all
the information we need to get started.<br>
<br>
<li>A more detailed discussion of the information you might need is in
the document "<a
 href="https://sourceforge.net/docman/display_doc.php?docid=20560&amp;group_id=47774"
 name="Writing A Simple Plug-in" type="text/html">Writing a Simple
Plug-in</a>". <br>
</li>
<h4>Defining a basic plug-in</h4>
<ul>
  <li>We will call the Disputatio plugin "Disputatio". Type that into
the "Plugin Name" field.</li>
  <li>The Java name for the plugin will be the reverse of your
institution's DNS domain followed plugin followed by a class name. For
example, ours will be called <span style="font-weight: bold;">org.lockss.plugin.disputatio.DisputatioPlugin</span>.
Click on the NONE and type it, substituting your DNS name for <span
 style="font-weight: bold;">org.lockss</span>.<br>
  </li>
  <li>We are defining version 1 of the Disputatio plugin, so make sure
the version field is 1.</li>
  <li><img src="configuration.gif" title=""
 alt="Default configuration window" style="width: 480px; height: 250px;"
 align="right">Now click on the row of dots beside "Configuration
Parameters". A
window pops up looking like this: In this case, the only
parameters we need the
administrator to define are the <span style="font-weight: bold;">base_url</span>,
which is
always required so that the plugin can find the journal (it will be <span
 style="font-weight: bold;">http://disputatio.com/</span>), and the <span
 style="font-weight: bold;">year</span>
(it will be <span style="font-weight: bold;">2004</span>, etc.), so it
can find the
publisher
manifest pages (at <span style="font-weight: bold;">http//disputatio.com/lockss2004.html</span>,
etc.). Select <span style="font-weight: bold;">year</span> and click
Add.<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="configuration2.gif" title=""
 alt="Configuration window filled in"
 style="width: 480px; height: 250px;" align="right">The window should
look like this: Now click OK.<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="url-template.gif" title=""
 alt="URL template initial state" style="width: 380px; height: 315px;"
 align="right">Now we use these
parameters to define the Start URL Template, which
tells the
plugin where to find the publisher manifest page. Click on the NONE
beside it. A template editor
window pops up, looking like this:<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="url-template2.gif" title=""
 alt="URL Name template filled in" style="width: 380px; height: 315px;"
 align="right">Click "Insert Parameter", click
in the upper box, type
lockss, pull down to <span style="font-weight: bold;">year</span>,
click "Insert Parameter" and the window
looks like this:
The string in the upper box is a printf format string into which the
parameters in the lower box are
substituted to obtain, in our case, <span style="font-weight: bold;">http://disputatio.com/lockss2004.html</span>
etc. Details on printf format strings can be found <a
 href="http://www.mkssoftware.com/docs/man1/printf.1.asp">here</a>.
Click Save.<br clear="all">
  </li>
</ul>
<br>
<ul>
</ul>
<ul>
  <li><img src="au-name.gif" title=""
 alt="AU Name template initial state"
 style="width: 380px; height: 315px;" align="right">Next we specify the
name that each AU
captured
by this plugin will have in the LOCKSS Administrative user interface.
Click on the NONE beside AU Name Template to get a template editor
window that looks like this:<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="au-name2.gif" title="" alt="AU Name Template filled in"
 style="width: 380px; height: 315px;" align="right">We want
the name of the 2004 AU to be <span style="font-weight: bold;">http://disputatio.com
2004</span>, i.e. the <span style="font-weight: bold;">base_url</span>
followed by a space
followed by the <span style="font-weight: bold;">year</span>.
Click&nbsp; "Insert Parameter", click in the upper box, type a space,
pull-down the menu beside "Insert Parameter" and choose <span
 style="font-weight: bold;">year</span>, click "Insert Parameter" and
the
window looks like this:&nbsp;
The <span style="font-weight: bold;">base_url</span> parameter is
substituted
for the %s and the <span style="font-weight: bold;">year</span> for
the %d to achieve what we want. Click Save.<br clear="all">
    <br>
  </li>
</ul>
<ul>
  <li>Next we need to specify a series of crawl rules that are applied
in order. Click on the row of dots beside to get a window that looks
like this:<img src="crawl-template.gif" title=""
 alt="Crawl rule template initial state"
 style="width: 440px; height: 140px;" align="right"> XXX the columns of
this window should be labeled Action and Pattern XXX Two rules are
pre-specified for
you; they will be needed in almost every case. The first says that
anything that doesn't start with base_url (in our case, <a
 href="http://disputatio.com/">http://disputatio.com/</a>) will
be excluded. The second says that the
publisher manifest page will be included. The rules are in the form of
a printf format string which has the configuration parameters for the
AU
substituted into it before being used as a regular expression pattern.
Regular expressions are full of pitfalls for the unwary. They use many
meta characters with special meanings (the ^ at the beginning of the
first rule is an example, it forces the pattern to be matched only if
it starts at the beginning of the URL). So the combination of format
string and regular expression would be hard to type by hand. The tool
makes this easy by building them up in stages. If you really want the
gory details, look <a
 href="http://directory.google.com/Top/Computers/Programming/Languages/Regular_Expressions/FAQs,_Help,_and_Tutorials/">here</a>.<br>
  </li>
  <li>&nbsp;Now we
want to make sure that any articles linked from the manifest page are
included. A
typical URL we want to include is <a
 href="http://disputatio.com/articles/016-3.pdf">http://disputatio.com/articles/016-3.pdf</a>
- so we want the pattern to match the <span style="font-weight: bold;">base_url</span>
followed by the text <span style="font-weight: bold;">articles/</span>
followed by anything followed by <span style="font-weight: bold;">.pdf</span>.</li>
  <li><img src="crawl-template2.gif" title=""
 alt="Pattern editor initial state" style="width: 380px; height: 315px;"
 align="right">Click Add to get a default rule to edit. It will have
the
Include action, which is what we want, and the NONE pattern, which we
edit by clicking on it to get a window that looks like this:<br
 clear="all">
  </li>
</ul>
<ul>
  <li><img src="crawl-template3.gif" title="" alt="Crawl rule stage 1"
 style="width: 380px; height: 315px;" align="right">Click on "Insert
Parameter" to
start the pattern with <span style="font-weight: bold;">base_url</span>,
like this:<br clear="all">
  </li>
</ul>
<br>
<br>
<ul>
  <li><img src="crawl-template4.gif" title=""
 alt="Pattern editor stage 2" style="width: 380px; height: 315px;"
 align="right">Now pull down the menu by
"Insert Match" and choose "String Literal". Click "Insert Match" and a
box pops up into which you type the text you want to match, in the case
    <span style="font-weight: bold;">articles/</span>. Click OK and the
pattern editor window looks like this:<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="crawl-template5.gif" title=""
 alt="Pattern editor stage 4" style="width: 380px; height: 315px;"
 align="right">Next we want to match anything, so pull down the menu by
"Insert
Match", choose&nbsp; "Anything" and click "Insert Match". The pattern
editor window looks like:&nbsp;
Note the .* you just inserted. This is a regular expression made of two
meta characters. The period matches any single character, the star
means
any sequence of characters matching the previous character which,
because it is a period, matches anything. So .* matches any string of
characters, specifically in our case strings like <span
 style="font-weight: bold;">016-3</span>.<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="crawl-template6.gif" title=""
 alt="Pattern editor stage 5" style="width: 380px; height: 315px;"
 align="right">Next we want to match <span style="font-weight: bold;">.pdf</span>.
Pull down the menu by "Insert Match", choose "String Literal" and click
Insert Match. A box pops up into which you type <span
 style="font-weight: bold;">.pdf</span>. Click OK. The pattern editor
window now looks like:&nbsp;
Note the backslash (\) before the period before <span
 style="font-weight: bold;">pdf</span>. Period is a regular expression
meta character, so it must be escaped with a backslash beforehand if it
is to match an actual period. The tool generates these "escape"
meta characters automatically.<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="crawl-template7.gif" title=""
 alt="Crawl rule window with third rule"
 style="width: 440px; height: 140px;" align="right">Now we're done with
this rule, so click Save. The Crawl Rule
window now looks like this: <br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="base-window2.gif" title="" alt="Base window after edits"
 style="width: 370px; height: 293px;" align="right">We're done for now,
so click OK. The base plugin editor
window
now looks like:<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="test-crawl.gif" title=""
 alt="First stage of crawl rule test"
 style="width: 400px; height: 131px;" align="right">We are now ready to
start testing the plugin.&nbsp; Pull
down the
"Plugin" menu and choose "Test Crawl Rules ...". A window pops up that
allows you to provide the configuration parameters you decided earlier
that were needed to identify an AU. Type
values for the configuration parameters, in our case <span
 style="font-weight: bold;">base_url</span> http://disputatio.com/ and <span
 style="font-weight: bold;">year</span> 2004. Click "Check AU".<br
 clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="test-crawl2.gif" title="" alt="Before crawl rule test"
 style="width: 400px; height: 400px;" align="right">Now a window
looking like this pops up with pre-loaded fields for the publisher
manifest page you
want to start testing from, in our case
http://disputation.com/lockss2004.html, the Test Depth (a file
linked from the manifest page has a depth of 1, a page linked from that
page has a depth of 2, and so on) and Fetch Delay (the number of
seconds to pause between fetching pages): <br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="test-crawl3.gif" title="" alt="Results of test"
 style="width: 400px; height: 400px;" align="right">Click "Check URL"
to start the test. The test results appear in the scrolling text pane:<br
 clear="all">
  </li>
</ul>
<br>
<ul>
  <li>In our case the results pane ends
up containing <a href="disputatio1.txt">this</a>. <img
 src="crawl-template8.gif" title="" alt="Final crawl rules"
 style="width: 438px; height: 172px;" align="right">You will see that
our rules correctly included all the
articles
and correctly excluded http://lockss.stanford.edu/, but there were some
files excluded that should have been included. We need to add rules
including <span style="font-weight: bold;">base_url</span> followed by
anything followed by <span style="font-weight: bold;">.js</span> and <span
 style="font-weight: bold;">base_url</span> followed by <span
 style="font-weight: bold;">images/</span> followed by anything
followed by <span style="font-weight: bold;">.gif</span>. Click on the
row of dots by "Crawl Rules" and follow the same process you used to
insert the <span style="font-weight: bold;">articles/</span> rule to
insert these rules. The Crawl Rule window should look like: <br
 clear="all">
  </li>
</ul>
<br>
<ul>
  <li>Click OK and test this again. The results pane should contain <a
 href="disputatio2.txt">this</a>. We have correctly included the
articles and the other files.</li>
  <li>Disputatio publishes two issues per year, so checking every three
weeks is excessive. Set the New Content Crawl Interval to 13 weeks.<br>
  </li>
  <li>Finally, save your plugin as an XML file using Save or Save
As.&nbsp; Submit the
resulting file to the LOCKSS team for testing and inclusion
in the distribution.</li>
</ul>
<h4>Improving the Design of the Example Plugin</h4>
Although the plugin we just defined includes and excludes the correct
files, it does so by explicitly specifying the files on the publisher's
site to include. In most cases it will be better to explicitly specify
the files to exclude, and include everything else. This approach is
more likely to work as the publisher tweaks their site.<br>
Which files on the Disputatio site do we know should be excluded? The
only ones we are sure should be excluded are the "home page" (we know
this will change with every new issue) and the publisher manifest pages
for other AUs (e.g the 2004 AU should not include the 2003 publisher
manifest page). A better plugin design would have rules that:<br>
<ul>
  <li>Exclude everything not starting with the <span
 style="font-weight: bold;">base_url</span> <a
 href="http://disputatio.com/">http://disputatio.com/</a> (the first
default rule).</li>
  <li>Include the publisher manifest page for this AU (the second
default rule).</li>
  <li>Exclude the publisher manifest pages from other AUs, in our case
everything matching the <span style="font-weight: bold;">base_url</span>
followed by the string literal <span style="font-weight: bold;">lockss</span>
followed by a number followed by <span style="font-weight: bold;">.html</span>.
Although this will match the publisher manifest page for this AU, that
page has already been included so this exclusion rule will not affect
it.<br>
  </li>
  <li>Exclude the "home page" under both its aliases. First as <a
 href="http://disputatio.com/">http://disputatio.com/</a>, i.e. a
pattern starting with <span style="font-weight: bold;">base_url</span>
followed by the end of the string. Second as <a
 href="http://disputatio.com/index.html">http://disputatio.com/index.html</a>,
i.e. a pattern starting with <span style="font-weight: bold;">base_url</span>
followed by the string literal <span style="font-weight: bold;">index.html</span>.</li>
</ul>
Restart the plugin tool and fill in the fields as before until you get
to the Crawl Rules. As before, when you click on the row of dots by
Crawl Rules, the window that pops up will have the first two rules
pre-defined. Now add the extra exclusion rules outlined above:<br>
<ul>
  <li>Exclude the publisher manifest pages for other AUs by clicking
Add, pulling down the menu from Include and choosing Exclude. Click on
NONE and choose:</li>
  <ul>
    <li>Insert Match, Start</li>
    <li>Insert Match, String Literal, lockss</li>
    <li>Insert Match, Any Number</li>
    <li>Insert Match, String Literal, .html</li>
    <li>Save</li>
  </ul>
  <li>Exclude the home page under its http://disputatio.com/ alias by
clicking Add, pulling down the menu from Include and choosing Exclude.
Click on NONE and choose:</li>
  <ul>
    <li>Insert Match, Start</li>
    <li>Insert Parameter, base_url</li>
    <li>Insert Match, End</li>
    <li>Save</li>
  </ul>
  <li>Exclude the home page under its http://disputatio.com/index.html
alias by clicking Add, pulling
down the menu from Include and choosing Exclude. Click on NONE and
choose:</li>
  <ul>
    <li>Insert Match, Start</li>
    <li>Insert Parameter, base_url</li>
    <li>Insert Match, String Literal, index.html</li>
    <li>Insert Match, End</li>
  </ul>
</ul>
<br>
<ul>
  <li><img src="file:///tmp/crawl-template9.gif" title=""
 alt="Alternate crawl rules" style="width: 440px; height: 140px;"
 align="right">The Crawl Rule
window will look like this:&nbsp; Note
the backslashes before the periods in the .html string literals in the
patterns - the tool automatically inserts these to prevent the period
being interpreted as a regular expression meta character.<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li>The result of testing this set of rules is <a
 href="disputatio3.txt">this</a>. Only the publisher manifest page is
included, because the default
action if no rules are matched is to exclude the URL. We need to add a
rule to include everything that isn't being explicitly excluded. Click
Add. Click on NONE and
choose:</li>
  <ul>
    <li>Insert Match, Start</li>
    <li>Insert Parameter, base_url</li>
    <li>Insert Match, Anything</li>
  </ul>
</ul>
<br>
<ul>
  <li><img src="file:///tmp/crawl-template10.gif" title=""
 alt="Improved crawl rules" style="width: 441px; height: 164px;"
 align="right">Now the Crawl Rule
window will look like this:&nbsp; You
may need to expand the window to see it all XXX the list should scroll
if it doesn't all fit in the window XXX. The result of testing this set
of crawl rules is <a href="disputatio4.txt">this</a>, which is
correct, and less likely than the original example to break as
the publisher changes their site.<br clear="all">
  </li>
</ul>
&nbsp;<br>
<h4>A More Complex Example Journal</h4>
We will use <a href="http://www.shu.ac.uk/emls/emlshome.html">Early
Modern Literary Studies</a> (EMLS) as an example of a somewhat more
complex journal and thus a somewhat more complex plugin. Its home page
and the <span style="font-weight: bold;">base_url</span> is at <a
 href="http://www.shu.ac.uk/emls/">http://www.shu.ac.uk/emls/</a>. The
publisher manifest page for Volume 9 is at <a
 href="http://www.shu.ac.uk/emls/lockss-volume9.html">http://www.shu.ac.uk/emls/lockss-volume9.html</a>.
EMLS normally publishes every 4 months but it also publishes "special
editions" on an irregular schedule. Each volume has a table of contents
page, for example <a href="http://www.shu.ac.uk/emls/09-3/09-3toc.htm">http://www.shu.ac.uk/emls/09-3/09-3toc.htm</a>.
The articles in the volume are at URLs like <a
 href="http://www.shu.ac.uk/emls/09-3/finntabl.htm">http://www.shu.ac.uk/emls/09-3/finntabl.htm</a>,
i.e. the directory (folder) emls/09-3 contains the pages for volume 9
number 3. We will pretend that this plugin is being developed at
Sheffield Hallam University (<a href="http://www.shu.ac.uk">http://www.shu.ac.uk</a>)<br>
<br>
<ul>
  <li>Start the tool. Set the Plugin Name to <span
 style="font-weight: bold;">Early Modern Literary Studies</span> and
the Plugin ID to <span style="font-weight: bold;">uk.ac.shu.plugin.emls.emlsPlugin</span>.</li>
  <li>Pop up the Configuration Parameters dialog. Add volume to the set
of configuration parameters, we need it because it is part of the
publisher manifest page URL.</li>
  <li><img src="emls-url.gif" title="" alt="EMLS Start URL dialog"
 style="width: 380px; height: 315px;" align="right">Pop up the Start
URL dialog. Insert <span style="font-weight: bold;">base_url</span>,
type <span style="font-weight: bold;">lockss-volume</span>, insert <span
 style="font-weight: bold;">volume</span>, type <span
 style="font-weight: bold;">.html</span>.<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="emls-pad.gif" title="" alt="Numeric padding selection"
 style="width: 390px; height: 195px;" align="right">When you insert <span
 style="font-weight: bold;">volume</span> you will be asked to set the
padding. In this case we need "do not pad" to get, in our case, just
the 9.<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="emls-au-name.gif" title="" alt="EMLS AU name"
 style="width: 380px; height: 315px;" align="right">Pop up the AU Name
Template dialog. Insert base_url, type space <span
 style="font-weight: bold;">vol</span> space, insert volume and set the
padding to "no padding".&nbsp; The AU name will look like <span
 style="font-weight: bold;">http://www.shu.ac.uk/emls/ vol 9</span>.<br
 clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="emls-crawl1.gif" title="" alt="EMLS initial crawl rule"
 style="width: 440px; height: 140px;" align="right">Pop up the Crawl
Rules dialog. Include everything that matches Start followed by
base_url followed by volume padded with zeros to a field width of 2
followed by a literal string of <span style="font-weight: bold;">-</span>
followed by Any Number followed by a string literal of <span
 style="font-weight: bold;">/</span> followed by Anything followed by
End.<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="emls-crawl2.gif" title="" alt="First EMLS crawl test"
 style="width: 400px; height: 131px;" align="right">Now test this crawl
rule. Configure the AU with the base_url <a
 href="http://www.shu.ac.uk/emls/">http://www.shu.ac.uk/emls/</a> and
the volume 9. Start with the default depth of 1.<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li>The results of this test are <a href="emls1.txt">here</a>. Note
that the rules have included the table of contents pages for
the three normal issues of Volume 9, but not the table of contents page
for the special issue 13. They have also excluded some decorations that
should be collected. This test did not collect any articles, but we're
not yet sure whether the rules are to blame. The articles are linked
from a page linked from the publisher manifest page, so they will only
be collected at depth 2 or more.</li>
</ul>
<ul>
  <li><img src="emls-crawl4.gif" title=""
 alt="Including the special edition"
 style="width: 440px; height: 140px;" align="right">Pop up the Crawl
Rules dialog and add a rule that includes the special issue table of
contents for the special issue by including anything that matches Start
followed by <span style="font-weight: bold;">base_url</span> followed
by the string literal <span style="font-weight: bold;">si-</span>
followed by Any Number followed by the string literal <span
 style="font-weight: bold;">/</span> followed by Anything followed by
End.<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="emls-crawl3.gif" title="" alt="EMLS crawl rule test"
 style="width: 400px; height: 400px;" align="right">Now test these
crawl rules. Configure the AU with the base_url <a
 href="http://www.shu.ac.uk/emls/">http://www.shu.ac.uk/emls/</a> and
the volume 9. Set the depth to 2. The test will take a while as it
waits the 6 seconds between fetching each page. The results are <a
 href="emls2.txt">here</a>.<br clear="all">
  </li>
</ul>
<br>
<br>
<ul>
  <li><img src="emls-crawl6.gif" title="" alt="Addition rule for .gif"
 style="width: 440px; height: 140px;" align="right">Note that we are
still excluding some .gif files that should be included. Pop up the
Crawl Rules dialog and add a rule that matches <span
 style="font-weight: bold;">base_url</span> followed by Anything
followed by the string literal <span style="font-weight: bold;">.gif</span>
followed by End,<br clear="all">
  </li>
</ul>
<br>
<ul>
  <li><img src="emls-crawl5.gif" title=""
 alt="Third test of EMLS crawl rules"
 style="width: 400px; height: 400px;" align="right">Now test these
crawl rules again. Configure the AU with the base_url <a
 href="http://www.shu.ac.uk/emls/">http://www.shu.ac.uk/emls/</a>
and the volume 9. Set the depth to 3. The test will take 5 minutes or
so as it
waits the 6 seconds between fetching each page. The results are <a
 href="emls3.txt">here</a>.<br clear="all">
  </li>
</ul>
<br>
Reviewing this rather long list, it appears that the crawl rules are
now including and excluding the correct files. Note that among the
files included are programs for Windows and the Mac, .gif and .jpg
images and .wrl files. XXX which are what? XXX<br>
<h4>What are the plugin generator's limits?</h4>
The plugin generation tool has limitations; the plugins for many more
complex journals will exceed them. In general, any plugin that requires
knowledge of the Java classes in the daemon is likely to fall into this
class. Although the "Expert Mode" described in the next section allows
plugins to use some pre-defined Java classes, we aren't yet ready to
explain how you find out what these classes are or what exactly they do
for you. We are still working to expand the capabilities of these
pre-defined classes as we build and test plugins for complex journals;
anyone else attempting a complex journal is also likely at present to
find their capabilities inadequate and need to add to them by writing
Java.<br>
<br>
There are a number of warning signs that a journal is complex enough to
be beyond the capabilities of the tool. If you find any of these
features in your chosen journal you should consult the LOCKSS team:<br>
<ul>
  <li>Some journals add advertisements, personalizations and other
dynamic content to the otherwise static pages. The LOCKSS daemon must <span
 style="font-style: italic;">filter</span> the pages of these journals
to remove the additions before comparing them with the same pages at
other caches, which will have received different advertisements, etc.
Filtering requires the use of special filter classes.</li>
  <li>Some journals experience massive spikes of reader interest
immediately a new issue is published. They typically want LOCKSS not to
crawl during these predictable periods of high load, and the LOCKSS
daemon has classes that provide suitable <span
 style="font-style: italic;">crawl windows</span> in time.</li>
  <li>Some journals do not return normal HTTP error codes, such as 404
for "Page not found" but instead return "helpful" pages with a
different return code. The LOCKSS daemon has classes that recognize
these error pages and re-map them to be conventional errors.</li>
  <li>Some journals have sophisticated access control methods or
crawler traps to prevent theft of their valuable content.</li>
  <li>Some journals have media types for which LOCKSS support is not
yet available. Examples are Real Audio, XXX.</li>
</ul>
<h4><a name="ExpertMode"></a>Using Expert Mode</h4>
Selecting expert mode in the Plug-in Menu will bring up the remaining
parameters:
<ul>
  <li>Default Crawl Depth: When doing a new content crawl, how many
levels down should it go to check for changes.
The default is 1. It will only check the manifest page for new content.</li>
  <li>Crawl Window Class: The name of the class which encapsulates the
temporal crawl restrictions for the
publishers site.</li>
  <li>Filter Classes: The list of filters by mime-type for filtering
content before performing a hash.</li>
  <li>Crawl Exception Class: The name of the class to be called when
HTTP
errors are generated. This only
needs to be implemented if the site uses HTTP return codes in
non-standard ways.</li>
  <li>Cache Exception Map: A map of return codes to error handlers.
This
only needs to be used if a return code
is being remapped on the site. So returning 404 for some 200 types.</li>
</ul>
<h4>Over-riding plugin settings</h4>
Most of the information in a plugin is fixed, but some items can be
over-ridden by the LOCKSS daemon's property settings, obtained locally
or from one of the LOCKSS property servers. The list of such
information is:<br>
<ul>
  <li>The refetch depth, which is the depth from the starting URL to
which the crawler's search for new content will proceed. If each new
article is linked from the publisher's manifest page directly,&nbsp;
this depth should be 1. If the manifest page links to the table of
content for an issue,&nbsp; which links to each new article this depth
should be 2, and so on. Each time it looks for new content, up to this
depth from the start URL, the crawler will re-fetch each URL even if it
is already in the cache.</li>
  <li>The new content crawl interval.</li>
  <li>A flag that enables and disables the crawl window (but not the
crawl window itself).</li>
  <li>The set of pre-configured titles.</li>
</ul>
<h4><br>
</h4>
<br>
</body>
</html>
